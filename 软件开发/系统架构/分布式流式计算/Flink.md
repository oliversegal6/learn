# Flink

Flink核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink提供了诸多更高抽象层的API以便用户编写分布式任务：

DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。

DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。

Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。

此外，Flink还针对特定的应用领域提供了领域库，例如：

Flink ML，Flink的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。

Gelly，Flink的图计算库，提供了图计算的相关API及多种图计算算法实现



分布式领域，计算和存储一直是两大子领域

Flink是一种用于有状态并行数据流式处理的分布式系统。一个Flink setup由分布在多台机器上的多个进程组成。分布式系统需要解决的常见挑战是集群中计算资源的分配和管理、进程协作、持久和可用数据存储，以及故障恢复。Flink本身并没有实现所有需要的功能。相反，它专注于其核心功能——分布式数据流式处理——并利用现有的集群基础设施和服务。



## **为什么选择 Flink**

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink6.png)

Flink 是一个开源的分布式流式处理框架：

1. 提供准确的结果，甚至在出现无序或者延迟加载的数据的情况下。

2. 它是状态化的容错的，同时在维护一次完整的的应用状态时，能无缝修复错误。
3. 大规模运行，在上千个节点运行时有很好的吞吐量和低延迟。

更早的时候，我们讨论了数据集类型（有界 vs 无穷）和运算模型（批处理 vs 流式）的匹配。Flink 的流式计算模型启用了很多功能特性，如状态管理，处理无序数据，灵活的视窗，这些功能对于得出无穷数据集的精确结果是很重要的。

除了提供数据驱动的视窗外，Flink还支持基于时间，计数，session等的灵活视窗。视窗能够灵活的触发条件定制化从而达到对复杂的流传输模式的支持。Flink的视窗使得模拟真实的创建数据的环境成为可能。

## 应用场景

### 事件驱动应用程序

事件驱动的应用程序是有状态的流应用程序，它接收事件流并在接收的事件上应用业务逻辑。根据业务逻辑的不同，事件驱动的应用程序可以触发一些操作，比如发送警报或者电子邮件，或者是将事件写入到输出事件流，该输出事件流可能会被其他事件驱动应用程序作为输入事件流而消费。

事件驱动应用程序的典型用例包括：

- 实时推荐，例如，当顾客浏览零售商的网站时推荐产品
- 模式检测或复杂事件处理(CEP)，例如，用于信用卡交易中的欺诈检测
- 异常检测，例如，检测侵入计算机网络的企图

事件驱动的应用程序是前面讨论的微服务的演变。它们通过事件日志而不是REST调用进行通信，并将应用程序数据保存为本地状态，而不是将其写入外部数据存储中或是从外部数据存储中读取数据，这类数据存储包括事务性数据库或键值存储

事件驱动的应用程序是一种有趣的设计模式，因为它与传统的独立出存储和计算的分层的体系架构或者目前流行的微服务体系架构相比，它们提供了一些优点。与对远程数据存储进行读写查询相比，本地状态访问，即从内存或本地磁盘进行读写操作，提供了非常好的性能。缩放和容错不需要特别考虑，因为这些方面是由流处理器处理的。最后，通过利用事件日志作为输入源，应用程序的完整输入可以可靠地存储并以确定的顺序重播。这是非常有吸引力的，特别是与Flink的保存点（savepoint）特性相结合，该特性可以将应用程序的状态重置为之前的某一一致性的保存点。通过重置(可能已被修改)应用程序的状态并重播输入事件，可以帮助我们修复应用程序的Bug并纠正/订正其影响，部署新版本的应用程序而不丢失其状态，或者运行what-if或A/B测试。

### 数据管道和实时ETL

IT体系结构包括许多不同的数据存储，例如关系和专用数据库系统、事件日志、分布式文件系统、内存缓存和搜索索引。所有这些系统都以不同的表示形式和数据结构存储数据，以便为它们的特定目提供最佳性能。组织/机构的数据的子集存储在多个系统中。例如，网上商城提供的产品信息可以存储在事务性数据库、web缓存和搜索索引中。由于数据的这种复制，数据存储之间必须保持同步。

传统的方法是使用一个定期的ELT，在存储系统之间移动数据，而这种方法通常不能足够快地传播更新。相反，一种常见的方法是将所有的更改写入到作为事实（truth，可以理解为真实数据的意思）数据来源的事件日志中。事件日志将这些更改发布给消费者，这些消费者会将这些更改合并到受影响的数据存储中。根据用例和数据存储，通常在合并之前需要对数据进行处理。例如，需要对它们进行规范化、与额外数据连接起来，或预先聚合，例如：ETL处理过程通常执行的转换。

低延迟的摄入、转换和插入数据是有状态的流处理应用程序的另一个通用使用场景。我们将这种类型的应用程序称为数据管道。数据管道的额外要求是能够在短时间内处理大量数据，即，支持高吞吐量和扩展应用程序的能力。操作数据管道的流处理器还应该支持各种source connector和sink connector，以便与各种存储系统以不同的数据格式进行数据读写操作。同样，Flink提供了操作数据管道所需的所有特性，并包含许多连接器。

### 流分析

流应用程序将结果存储在支持高效更新的外部数据存储中，例如数据库或键值存储。此外，Flink还提供了一个名为可查询状态【queryable state】的特性，允许用户将应用程序的状态作为键查找表【key-lookup table】公开开来，并允许外部应用程序访问它。流分析应用程序的实时更新结果可用于为仪表板【dashboard】应用程序

运行有状态流程序的流处理器会负责所有的处理步骤，包括事件摄入、持续计算(包括状态维护)和更新结果。此外，流处理器负责以唯一状态一致性【 exactly-once state consistency】保证从故障中恢复，而且应该能够调整应用程序的并行性。支持流分析应用的额外需求是：一个是支持按照事件时间【event-time】处理事件，即保证事件处理的顺序，以便产生正确和确定的结果，另一个是需要在短时间内处理大量数据的能力，即高吞吐量。Flink为所有这些需求都提供了完美的答案。

流分析应用程序的典型用例是:

1. 监控手机网络的质量。
2. 分析移动应用程序中的用户行为。
3. 以消费者技术实现实时数据的特别分析。
   虽然本书没有涉及，但值得一提的是Flink还提供了对流的SQL查询分析的支持。多家公司已经基于Flink的SQL支持构建了流分析服务，既可以用于内部使用，也可以公开提供给付费用户。



## 数据流编程介绍(dataflow programming)



### 数据流图[Dataflow graphs]



数据流程序描述了数据如何在业务操作【operations】之间流动。数据流程序通常表示为有向图，其中节点称为运算子/操作符【operator】，表示计算，而边表示数据依赖及数据流向。没有输入端口的运算子/操作符【operator】称为数据源[data source]，没有输出端口的运算子/操作符【operator】称为数据接收器[data sinks]

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink7.png)



上图是逻辑图，是因为它传达的是计算逻辑的高级视图，为了执行一个数据流程序，需要将其逻辑数据流图转换为物理数据流图，其中包括关于如何执行计算的详细信息。例如，如果我们使用分布式处理引擎，每个运算子/操作符【operator】可能在不同的物理机器上运行多个并行任务。

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink8.png)

在逻辑数据流图中，节点表示运算子/操作符【operator】，而在物理数据流中，节点表示任务【tasks】。“Extract hashtags”和“Count”运算子/操作符【operator】分别有两个并行的运算子/操作符任务【operator task】，每个运算子/操作符任务【operator task】均是对输入数据的子集执行计算。

### 数据并行与任务并行

可以以不同的方式利用数据流图中的并行性。首先，您可以对输入数据进行分区，并让相同操作的任务并行地在数据子集上执行。这种类型的并行性称为**数据并行性**。数据并行性非常有用，因为它允许处理大量数据并将计算负载分散到多个计算节点。其次，可以让来自不同运算子/操作符【operator】的任务并行地对相同或不同的数据执行计算。这种并行性称为**任务并行性**。使用任务并行性可以更好地利用集群的计算资源。

### 数据交换策略

数据交换策略定义了如何将数据项/数据条目分配给物理数据流图中的任务。数据交换策略可以由执行引擎根据运算子/操作符【operator】的语义自动选择，也可以由数据流程序员显式地强制实施。

- 转发【forward】策略将数据从一个任务发送到另一个接收任务。如果两个任务位于同一台物理机器上(通常由任务调度器保证)，那么这种交换策略可以避免网络通信。
- 广播【broadcast】策略将每个数据项发送给一个运算子/操作符【operator】的所有并行的任务上。因为这种策略涉及数据复制和网络通信，所以成本相当高。
- 基于键【key-based】的策略通过键属性【key】对数据进行分区，并确保具有相同key的数据项将由相同的任务处理。在图2.2中，“Extract hashtags”运算子/操作符【operator】的输出基于key(hashtag)进行分区，这样count运运算子/操作符任务【operator task】就能够正确计算每个hashtag的出现次数。
- 随机【random】策略将数据项均匀地分配给运算子/操作符任务【operator task】，以便在任务之间均匀地分配负载。
  THE FORWARD AND RANDOM STRATEGIES AS KEY-BASED
  转发策略和随机策略也可以看作是基于键的策略的变体，前者保持上游元组的键，而后者则执行键的随机重新分配。

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink9.png)

## Flink基本概念

### Flink任务处理过程

当Flink系统启动时，首先启动JobManager和一至多个TaskManager。JobManager负责协调Flink系统，TaskManager则是执行并行程序的worker。当系统以本地形式启动时，一个JobManager和一个TaskManager会启动在同一个JVM中。
当一个程序被提交后，系统会创建一个Client来进行预处理，将程序转变成一个并行数据流的形式，交给JobManager和TaskManager执行。

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink1.webp)

- JobManager是控制单个应用执行的主进程，即，每个应用程序都是由不同的JobManager控制的。JobManager接收一个应用用于执行。该应用由一个所谓的JobGraph、一个逻辑数据流图【 logical dataflow graph】和一个打包了所有必需的类、库和其他资源的JAR文件组成。JobManager将JobGraph转换为物理数据流图【physical dataflow graph】，我们称该物理数据流图为ExecutionGraph，它包含可以并行执行的任务。JobManager要求ResourceManager提供执行任务所需的资源(即TaskManager slots)，一旦它接收到足够的TaskManager插槽，它就将任务分配给这些负责执行任务的TaskManager 。在执行期间，JobManager负责所有需要中枢协调的操作，例如检查点的协调(请参阅后面的部分)。
- Flink针对不同的环境和资源提供者(如YARN、Mesos、Kubernetes和stand-alone部署)提供了多种ResourceManager的实现。ResourceManager负责管理TaskManager插槽，它是Flink处理资源的单元。当JobManager请求申请TaskManager 插槽时，ResourceManager要求具有空闲插槽的TaskManager将这些插槽提供给JobManager。如果没有足够的插槽来满足JobManager的请求，则ResourceManager可以与资源提供者(如Apache Mesos、YARN和Kubernetes)对话，以便提供可以用于启动TaskManager进程的容器。ResourceManager还负责杀死空闲的任务管理器来释放计算资源。
- TaskManagers是Flink的工作进程。通常，在Flink setup中运行着多个TaskManagers。TaskManagers提供了一定数量的插槽。插槽的数量限制了TaskManager可以执行的任务的数量。启动TaskManagers之后，TaskManagers会在ResourceManager中注册它的插槽。在ResourceManager的指示下，TaskManager会向JobManager提供一个或多个插槽。然后JobManager可以向插槽分配任务来执行它们。在执行期间，TaskManager可以与运行的是相同应用的任务的其他TaskManager交换数据。任务的执行和插槽的概念将在后面的部分中详细讨论。
- Dispatcher的运行贯穿各个执行的作业，并提供一个REST接口用于提交应用程序以供执行。一旦它接收到应用程序，它就启动JobManager并将应用程序移交给它。REST接口使Dispatcher能够作为防火墙保护下的集群的入口点。Dispatcher还运行一个web仪表板来提供关于以往作业执行的详细信息。根据如何提交应用程序以供执行，Dispatcher有时并不适用。

![Flink架构](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink10.png)

### 高吞吐量和低延迟

通过网络连接发送单个数据记录是很低效的，并且会造成很大的性能花销。缓冲是充分利用网络连接带宽的一种强制性技术。在流处理上下文中，缓冲的一个缺点是它增加了延迟，因为数据记录是首先被收集到缓冲区中，而不是立即被分发。如果发送端任务很少为特定的接收端任务生成数据记录，则可能需要很长的时间才能填充满缓冲器，然后再发送相应的缓冲区。因为这将导致较高的处理延迟，所以Flink确保每个缓冲区在一段时间后被发送，而不管它被填充了多少。这个超时可以解释为网络连接所增加的延迟的上限。但是，该阈值并不作为作业的绝对延迟SLA，因为作业可能涉及多个网络连接，而且它也不考虑实际处理所造成的延迟。



### 背压流量控制

当流式应用程序接收的是一个高容量的数据流时，那么很容易就会出现任务处理速度跟不上数据流到达速度的情况。如果输入流的容量对于分配给某个操作符的资源量来说过高，或者操作符的输入速率显著变化并导致高负载峰值，就可能发生这种情况。无论操作符为什么不能处理其数据输入，这种情况都不应该成为流处理器终止应用程序的原因。相反，流处理器应该优雅地将流应用程序的数据输入速率控制在应用程序可以处理数据的最大速度。通过适当的监控基础设施，可以很容易地检测到节流情况，通常可以通过添加更多的计算资源和增加瓶颈操作符的并行度来解决节流问题。所述的流量控制技术称为背压



1. 当应用程序的数据输入速率增加时，发送端任务可以承受这种负载，但是接收端任务开始落后，不再能够按照记录到达的速率处理记录。现在接收端的TaskManager开始使用缓冲池中的缓冲区来缓冲接收到的数据。在某一时刻，当接收端TaskManager的缓冲池耗尽时，就无法继续缓冲到达的数据。
2. 发送端TaskManager 也开始用缓冲池中的缓冲区来缓冲输出数据，直到它自己的缓冲池为空为止。
3. 最后，在有新的可用缓冲区出现之前，发送端任务都会阻塞，且不能发射更多的数据。被阻塞任务由于其接收者较慢的接收行为，使得其自身也变成一个较慢的接收者，并反过来影响其上游的任务。这种放缓会逐渐升级到流式应用程序的发生器任务上【source】。最终，整个应用程序被减慢到最慢的操作符的处理速度。



### 流的时间有序性保证

数据记录的时间戳之外，一个基于事件时间的Flink流式应用程序还必须提供watermark对数据重排序，来保证整体数据流的有序性

分段数据的重排序，依靠的是数据流的watermark值。每当我们每接收到一份数据到buffer中时，我们选定其中最新的watermark值，对buffer里数据的时间小于此watermark值的数据在buffer中做一个排序。然后将此排序好的数据发向下游。这里基于的一个原则是：时间比当前watermark消息早的数据都已经到来了，所以我们可以大胆地把这批数据先拍好序再发出去

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink2.png)



### 窗口序列对齐

Flink流任务中，会涉及到数据被多次窗口处理的问题，比如数据流被A窗口处理过有到看B窗口中做处理。我们如何来指定窗口的序列关系呢？

这里Flink采用了一种窗口逐一对齐的做法。后一窗口的起始末尾边界与前一序处理窗口的边界完全对齐，对应区间范围内的结果数据同样落位到相对应的区间窗口内。如下图所示

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink3.png)

### State Management

在Flink中，状态总是与特定的操作符相关联。为了使Flink的运行时【runtime】知道操作符的状态，操作符需要注册其状态。有两种类型的状态，操作符状态【Operator State】和键控状态【Keyed State】，它们可以从不同的作用域访问

#### 操作符状态【Operator State】

操作符状态【Operator State】的作用域是操作符任务。这意味着由同一并行任务处理的所有记录都可以访问相同的状态。操作符状态不能被相同或不同操作符的其他任务访问

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink12.png)

Flink为操作符状态提供了三个原语：

- List State ：列表状态将状态表示为条目列表。
- Union List State：联合列表状态也将状态表示为条目列表。它与常规列表状态的不同之处在于，在发生故障或应用程序从保存点启动时，它是如何恢复的。我们将在本章后面讨论这种差异。
- Broadcast State ：广播状态是为操作符的每个任务的状态相同的特殊情况而设计的。此属性/特性可在检查点期间或者重新缩放操作符时使用。这两个方面将在本章后面几节中讨论。

#### Keyed State

键控状态的作用域限定在操作符输入流的数据记录上定义的键。Flink为每个键值维护一个状态实例，并将具有相同键的所有记录分区到维护该键状态的操作符任务上。当一个任务处理一条记录时，它会自动将状态访问范围限定到当前记录的键。因此，具有相同键的所有记录访问的是相同的状态

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink13.png)

您可以将键控状态看作键值对映射，它在一个操作符的所有并行任务之上，按照键进行分区(或切片)。Flink为键控状态提供了不同的原语，这些原语决定了为这个分布映射中的每个键所存储的值的类型。我们将简要讨论最常见的键控状态原语。

- Value State：值状态为每个键存储一个任意类型的值。当然，复杂的数据结构也可以存储为值状态
- List State：列表状态存储每个键的值列表。列表中的条目可以是任意类型的。
- Map State：映射状态为每个键存储一个键值对映射。该映射的键和值可以是任意类型的。

#### State Backends

有状态的操作符的任务通常为每个传入数据读取和更新它的状态。由于高效的状态访问对于低延迟的处理记录至关重要，因此每个并行任务都会在本地维护其状态，以确保本地状态访问。状态的存储、访问和维护的具体方式由称为State Backends的可插拔组件决定。State Backends主要负责两个方面，本地状态管理和指向远程位置的检查点状态。

对于本地状态管理，State Backends确保键控状态的作用域正确地被限定到当前键，并存储和访问所有键控状态。Flink提供的State Backends通过将键控状态作为对象，存储在JVM堆内存数据结构中来管理键控状态。还有一种State Backends，它会序列化状态对象，并将它们放入RocksDB，RocksDB将它们写入本地硬盘。虽然第一个选项提供非常快速的状态访问，但它与内存大小有关。访问由RocksDB State Backends存储的状态比较慢，但是它的状态可以增长得非常大

状态检查点非常重要，因为Flink是一个分布式系统，而状态仅在本地维护。TaskManager进程(及其上运行的所有任务)在任何时候都可能失败，因此必须将其存储视为不稳定的。State Backends负责将任务的状态检查点指向远程持久存储。检查点的远程存储可以是分布式文件系统，也可以是数据库系统。State Backends在状态的检查点方式上有所不同。例如，RocksDB State Backends支持异步和增量检查点，这大大降低了状态非常大的检查点开销。

### 流数据的容错：Checkpoint, Savepoints, and State Recovery

流处理与批处理相比，它的一大优势在于它的低延时，而批处理的一个得天独厚的优势是错误恢复容易。因为批处理任务在每次的批处理操作中会保存住全部的输入数据，如果出现结果算错的情况，重新执行一次处理过程即可。而流式计算中连续不断的数据处理，使得错误恢复变得复杂起来。所以假如流处理任务能够做到快速的错误恢复，那么其可用性将会大大加强。

Flink为了实现定期的checkpoint，做的一个核心改动是在流数据中增加一个标记数据记录，名为stream barrier。不同时间点插入barrier数据将流数据分隔成了多份，每份对应一次checkpoint操作，同时checkpoint会保留住数据源source当时的偏移量信息。如下图所示：

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink4.png)

当barrier标记从source流向到sink下游，并且系统受到sink端的确认消息后，此checkpoint宣告正式完成。如果过程中需要涉及多input的输入时，处理快的barrier流会在过程中等待落后的其它流直到它们的barrier信息到来，然后再往下游传输数据，如下图

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink5.png)

对于应用中所涉及的中间状态数据，Flink支持用户自定义状态持久化操作，然后应用程序在重新启动的时候从外部存储中重新恢复状态数据。

一般情况下，为了保证状态数据的一致性，checkpoint状态数据的时候是同步的过程。Flink在后来实现了一种异步状态同步的方法，主要采用的思路是拷贝原状态的数据，然后用异步线程去持久化拷贝的那份状态数据。同时为了防止每次checkpoint大量相同状态的数据，Flink在后期也实现了增量checkpoint的功能。

## 高可用集群

JobManager协调每一个Flink集群环境，它负责作业调度和资源管理。默认情况下，一个Flink集群中只有一个JobManager实例，这很容易造成单点故障（SPOF）。可以配置Standalone模式和YARN集群模式下的高可用JobManager的HA，meta信息管理是通过Zookeeper实现的。

### Standalone集群模式

对于Standalone集群模式下的JobManager高可用通常的方案是：Flink集群的任一时刻只有一个leading JobManager，并且有多个standby JobManager。当leader失败后，standby通过选举出一个JobManager作为新的leader

## YARN集群模式

YARN集群我们不需要运行多个JobManager（ApplicationMaster）实例，只需要运行一个实例，如果失败了通过YARN来进行重启

### JobManager failures

Flink的高可用模式基于Apache ZooKeeper，该系统用于需要协调和一致的分布式服务。Flink使用ZooKeeper进行领导者选举【leader election】，并将其作为一个高可用且可持久化的数据存储。在高可用模式下操作时，JobManager将JobGraph和所有必需的元数据(如应用程序的JAR文件)写入到由state backend配置的远程存储系统上。此外，JobManager还会将指向存储位置的指针写到ZooKeeper的数据存储中。在应用程序执行期间，JobManager接收各个任务检查点的状态句柄(存储位置)。在检查点完成后，即，当所有任务都成功地将状态写入远程存储时，JobManager将状态句柄写入远程存储，并将指向该位置的指针写入ZooKeeper。因此，从JobManager故障中恢复所需的所有数据都存储在远程存储中，ZooKeeper保存了指向该存储位置的指针

![](E:\github\learn\软件开发\系统架构\分布式流式计算\pic\flink11.png)

当JobManager发生失败时，归属于其应用程序的所有任务都会自动取消。接管故障主机工作的新JobManager将会执行以下步骤。

1. 它从ZooKeeper请求存储位置以用来从远程存储中获取JobGraph、JAR文件和应用程序最后一个检查点的状态句柄。
2. 它从ResourceManager请求处理插槽以继续执行应用程序
3. 它重新启动应用程序，并将所有任务的状态重置为最后一个完成的检查点时的状态。

将应用程序作为库部署在容器环境(如Kubernetes)中时，发生故障的JobManager容器或TaskManager容器可以自动重启。当在YARN或Mesos上运行时，Flink的其余进程将触发JobManager或TaskManager进程的重启。当Flink以stand-alone集群模式运行时，Flink没有提供任务用于重启故障进程的工具。因此，运行备用的JobManager和TaskManager以用于接管故障组件的工作