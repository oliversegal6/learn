---
title: MicroService
date: 2019-05-24 22:06:47
categories: 
- 软件开发
- 系统架构
- 分布式架构
---

### 微服务架构监控

微服务架构虽然解决了目前诸多的架构层面的问题，但在分布式部署的环境中，如何才能够有效监控每一个服务，并及时发现系统中的问题又成为了新的挑战

在APM领域，程序的细节决定产品的性能。目前按照探针的实现，可行的思路主要有四种：

1. 基于日志系统，探针只负责对日志加上编号，又类似ELK的系统进行收集、处理、展示。这方面没有很成熟的产品，一般都属于公司内部封装的框架。

2. 自动探针，适用语言：Java、C#、PHP、Node.js等等存在VM的语言。绝对大多数的商业产品和热门的开源产品都属于这个系列。

3. 全手动探针，优势是适用范围广，最有名的就是Zipkin的整个生态系统，分布式追踪几乎无处不在。也是现在全球运用最广泛的分布式监控系统。

4. 同时支持自动和手动模式的探针，适用语言同样是Java、C#、PHP、Node.js等等存在VM的语言，由于技术复杂性提高，运用的较少。优点是入门方便，同时使用灵活。商业上主要是Instana，开源主要是sky-walking提供了技术解决方案。

#### 监控系统一般分为三大模块

**探针或SDK，负责数据采集和发送**。探针或SDK是应用程序的收集端。一般使用插件的模式，自动探针一般是不需要修改程序，而SDK则是需要修改部分配置或者代码。skywalking就是自动探针为主，zipkin-brave就是Zipkin的Java手动探针。

**Collector模块，负责数据收集、分析、汇总、告警和存储**。Collector模块，这个根据不同的APM实现，可能由一个或者多个子系统构成。Collector负责对探针和SDK提供网络接口（TCP、UDP、HTTP不同形式接口）。

功能上，主要包含数据收集、分析、汇总、告警和存储。这些模块在复杂的开源APM和商业产品上都保持一致。大家选择时，值得注意的是，现在包括Zipkin在内的不少国外的追踪系统，是只负责追踪，不负责分析汇总，他们认为分析可以由使用者自己负责。

作为分析方式，分为流式分析和异步批量两种，而流式分析会对数据统计和告警的实时性更有帮助。

**UI，负责高实时性的展现**。包括但不限于Trace的查询，统计数据展现，拓扑图展现，VM或进程相关信息等，监控关键数据的展现。

#### 服务注册、发现、负载均衡和健康检查

和单块(Monolithic)架构不同，微服务架构是由一系列职责单一的细粒度服务构成的分布式网状结构，服务之间通过轻量机制进行通信，这时候必然引入一个服务注册发现问题，也就是说服务提供方要注册通告服务地址，服务的调用方要能发现目标服务，同时服务提供方一般以集群方式提供服务，也就引入了负载均衡和健康检查问题。根据负载均衡LB所在位置的不同，目前主要的服务注册、发现和负载均衡方案有三种：

第一种是集中式LB方案，如下图Fig 1，在服务消费者和服务提供者之间有一个独立的LB，LB通常是专门的硬件设备如F5，或者基于软件如LVS，HAproxy等实现。LB上有所有服务的地址映射表，通常由运维配置注册，当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略（比如Round-Robin）做负载均衡后将请求转发到目标服务。LB一般具备健康检查能力，能自动摘除不健康的服务实例。服务消费方如何发现LB呢？通常的做法是通过DNS，运维人员为服务配置一个DNS域名，这个域名指向LB。
![Alt text](https://oliversegal6.github.io/images/microServiceP1.png)

集中式LB方案实现简单，在LB上也容易做集中式的访问控制，这一方案目前还是业界主流。集中式LB的主要问题是单点问题，所有服务调用流量都经过LB，当服务数量和调用量大的时候，LB容易成为瓶颈，且一旦LB发生故障对整个系统的影响是灾难性的。另外，LB在服务消费方和服务提供方之间增加了一跳(hop)，有一定性能开销。

第二种是进程内LB方案，针对集中式LB的不足，进程内LB方案将LB的功能以库的形式集成到服务消费方进程里头，该方案也被称为软负载(Soft Load Balancing)或者客户端负载方案，下图Fig 2展示了这种方案的工作原理。这一方案需要一个服务注册表(Service Registry)配合支持服务自注册和自发现，服务提供方启动时，首先将服务地址注册到服务注册表（同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查），服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询（同时缓存并定期刷新）目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。这一方案对服务注册表的可用性(Availability)要求很高，一般采用能满足高可用分布式一致的组件（例如Zookeeper, Consul, Etcd等）来实现。
![Alt text](https://oliversegal6.github.io/images/microServiceP2.png)


进程内LB方案是一种分布式方案，LB和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。但是，该方案以客户库(Client Library)的方式集成到服务调用方进程里头，如果企业内有多种不同的语言栈，就要配合开发多种不同的客户端，有一定的研发和维护成本。另外，一旦客户端跟随服务调用方发布到生产环境中，后续如果要对客户库进行升级，势必要求服务调用方修改代码并重新发布，所以该方案的升级推广有不小的阻力。

进程内LB的案例是Netflix的开源服务框架，对应的组件分别是：Eureka服务注册表，Karyon服务端框架支持服务自注册和健康检查，Ribbon客户端框架支持服务自发现和软路由。另外，阿里开源的服务框架Dubbo也是采用类似机制。

第三种是主机独立LB进程方案，该方案是针对第二种方案的不足而提出的一种折中方案，原理和第二种方案基本类似，不同之处是，他将LB和服务发现功能从进程内移出来，变成主机上的一个独立进程，主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立LB进程做服务发现和负载均衡，见下图Fig 3。
![Alt text](https://oliversegal6.github.io/images/microServiceP3.png)

该方案也是一种分布式方案，没有单点问题，一个LB进程挂了只影响该主机上的服务调用方，服务调用方和LB之间是进程内调用，性能好，同时，该方案还简化了服务调用方，不需要为不同语言开发客户库，LB的升级不需要服务调用方改代码。该方案的不足是部署较复杂，环节多，出错调试排查问题不方便。

该方案的典型案例是Airbnb的SmartStack服务发现框架，对应组件分别是：Zookeeper作为服务注册表，Nerve独立进程负责服务注册和健康检查，Synapse/HAproxy独立进程负责服务发现和负载均衡。Google最新推出的基于容器的PaaS平台Kubernetes，其内部服务发现采用类似的机制。
![Alt text](https://oliversegal6.github.io/images/microServiceP4.png)

#### 服务容错

当企业微服务化以后，服务之间会有错综复杂的依赖关系，例如，一个前端请求一般会依赖于多个后端服务，技术上称为1 -> N扇出(见图Fig 6)。在实际生产环境中，服务往往不是百分百可靠，服务可能会出错或者产生延迟，如果一个应用不能对其依赖的故障进行容错和隔离，那么该应用本身就处在被拖垮的风险中。在一个高流量的网站中，某个单一后端一旦发生延迟，可能在数秒内导致所有应用资源(线程，队列等)被耗尽，造成所谓的雪崩效应(Cascading Failure，见图Fig 7)，严重时可致整个网站瘫痪。

经过多年的探索和实践，业界在分布式服务容错一块探索出了一套有效的容错模式和最佳实践，主要包括：

1. 电路熔断器模式(Circuit Breaker Patten), 该模式的原理类似于家里的电路熔断器，如果家里的电路发生短路，熔断器能够主动熔断电路，以避免灾难性损失。在分布式系统中应用电路熔断器模式后，当目标服务慢或者大量超时，调用方能够主动熔断，以防止服务被进一步拖垮；如果情况又好转了，电路又能自动恢复，这就是所谓的弹性容错，系统有自恢复能力。下图Fig 8是一个典型的具备弹性恢复能力的电路保护器状态图，正常状态下，电路处于关闭状态(Closed)，如果调用持续出错或者超时，电路被打开进入熔断状态(Open)，后续一段时间内的所有调用都会被拒绝(Fail Fast)，一段时间以后，保护器会尝试进入半熔断状态(Half-Open)，允许少量请求进来尝试，如果调用仍然失败，则回到熔断状态，如果调用成功，则回到电路闭合状态。

![Alt text](https://oliversegal6.github.io/images/microServiceP5.png)

2. 舱壁隔离模式(Bulkhead Isolation Pattern)，顾名思义，该模式像舱壁一样对资源或失败单元进行隔离，如果一个船舱破了进水，只损失一个船舱，其它船舱可以不受影响 。线程隔离(Thread Isolation)就是舱壁隔离模式的一个例子，假定一个应用程序A调用了Svc1/Svc2/Svc3三个服务，且部署A的容器一共有120个工作线程，采用线程隔离机制，可以给对Svc1/Svc2/Svc3的调用各分配40个线程，当Svc2慢了，给Svc2分配的40个线程因慢而阻塞并最终耗尽，线程隔离可以保证给Svc1/Svc3分配的80个线程可以不受影响，如果没有这种隔离机制，当Svc2慢的时候，120个工作线程会很快全部被对Svc2的调用吃光，整个应用程序会全部慢下来。

3. 限流(Rate Limiting/Load Shedder)，服务总有容量限制，没有限流机制的服务很容易在突发流量(秒杀，双十一)时被冲垮。限流通常指对服务限定并发访问量，比如单位时间只允许100个并发调用，对超过这个限制的请求要拒绝并回退。

回退(fallback)，在熔断或者限流发生的时候，应用程序的后续处理逻辑是什么？回退是系统的弹性恢复能力，常见的处理策略有，直接抛出异常，也称快速失败(Fail Fast)，也可以返回空值或缺省值，还可以返回备份数据，如果主服务熔断了，可以从备份服务获取数据。

Netflix将上述容错模式和最佳实践集成到一个称为Hystrix的开源组件中，凡是需要容错的依赖点(服务，缓存，数据库访问等)，开发人员只需要将调用封装在Hystrix Command里头，则相关调用就自动置于Hystrix的弹性容错保护之下。Hystrix组件已经在Netflix经过多年运维验证，是Netflix微服务平台稳定性和弹性的基石，正逐渐被社区接受为标准容错组件。


#### 服务前端网关

微服务除了内部相互之间调用和通信之外，最终要以某种方式暴露出去，才能让外界系统（例如客户的浏览器、移动设备等等）访问到，这就涉及服务的前端路由，对应的组件是服务网关(Service Gateway)，

客户端直接与各个微服务通讯，会有以下的问题：

- 客户端会多次请求不同的微服务，增加了客户端的复杂性。
- 存在跨域请求，在一定场景下处理相对复杂。
- 认证复杂，每个服务都需要独立认证。
- 难以重构，随着项目的迭代，可能需要重新划分微服务。例如，可能将多个服务合并成一个或者将一个服务拆分成多个。如果客户端直接与微服务通讯，那么重构将会很难实施。
- 某些微服务可能使用了防火墙/浏览器不友好的协议，直接访问会有一定困难

网关是连接企业内部和外部系统的一道门，有如下关键作用：

1. 服务反向路由，网关要负责将外部请求反向路由到内部具体的微服务，这样虽然企业内部是复杂的分布式微服务结构，但是外部系统从网关上看到的就像是一个统一的完整服务，网关屏蔽了后台服务的复杂性，同时也屏蔽了后台服务的升级和变化。
2. 安全认证和防爬虫，所有外部请求必须经过网关，网关可以集中对访问进行安全控制，比如用户认证和授权，同时还可以分析访问模式实现防爬虫功能，网关是连接企业内外系统的安全之门。
3. 限流和容错，在流量高峰期，网关可以限制流量，保护后台系统不被大流量冲垮，在内部系统出现故障时，网关可以集中做容错，保持外部良好的用户体验。
4. 监控，网关可以集中监控访问量，调用延迟，错误计数和访问模式，为后端的性能优化或者扩容提供数据支持。
5. 日志，网关可以收集所有的访问日志，进入后台系统做进一步分析。

除以上基本能力外，网关还可以实现线上引流，线上压测，线上调试(Surgical debugging)，金丝雀测试(Canary Testing)，数据中心双活(Active-Active HA)等高级功能。

网关通常工作在7层，有一定的计算逻辑，一般以集群方式部署，前置LB进行负载均衡。

开源的网关组件有Netflix的Zuul，特点是动态可热部署的过滤器(filter)机制，其它如HAproxy，Nginx等都可以扩展作为网关使用。

在介绍过服务注册表和网关等组件之后，我们可以通过一个简化的微服务架构图(Fig 5)来更加直观地展示整个微服务体系内的服务注册发现和路由机制，该图假定采用进程内LB服务发现和负载均衡机制。在下图Fig 5的微服务架构中，服务简化为两层，后端通用服务（也称中间层服务Middle Tier Service）和前端服务（也称边缘服务Edge Service，前端服务的作用是对后端服务做必要的聚合和裁剪后暴露给外部不同的设备，如PC，Pad或者Phone）。后端服务启动时会将地址信息注册到服务注册表，前端服务通过查询服务注册表就可以发现然后调用后端服务；前端服务启动时也会将地址信息注册到服务注册表，这样网关通过查询服务注册表就可以将请求路由到目标前端服务，这样整个微服务体系的服务自注册自发现和软路由就通过服务注册表和网关串联起来了。如果以面向对象设计模式的视角来看，网关类似Proxy代理或者Façade门面模式，而服务注册表和服务自注册自发现类似IoC依赖注入模式，微服务可以理解为基于网关代理和注册表IoC构建的分布式系统

#### 微服务架构与单体架构的对比

对于简单项目来说，微服务架构。
**优势项**：系统设计（高内聚低耦合）、系统设计（扩展性）、需求变更响应速度、系统升级效率、知识积累、非功能需求、职责、成就感、风险
**劣势项**：上手难度、运维效率、开发效率、硬件需求、项目成本；

对于复杂项目来说，微服务架构。
**优势项**：硬件需求、项目成本、开发效率、系统设计（高内聚低耦合）、系统设计（扩展性）、需求变更响应速度、系统升级效率、知识积累、非功能需求、职责、成就感、风险
**劣势项**：上手难度、运维效率；

### 微服务架构的风险

微服务架构将业务逻辑分散到了各个微服务当中，微服务间通过网络层进行通信。网络通信带来了额外的延迟和复杂性，需要多个物理组件和逻辑组件共同协作。分布式系统的额外复杂性增加了出现网络故障的几率。

微服务架构相比单体架构最大的优势之一在于，不同的团队可以独立地设计、开发和部署他们的服务。他们可以完全掌控自己的微服务生命周期。当然，这也意味着他们无法控制服务依赖项，因为依赖项的控制权掌握在其他团队手中。在采用微服务架构时，我们要时刻铭记，发布、配置等方面的问题可能会导致服务提供者出现短暂的不可用。

**变更管理**

在微服务架构里，服务之间是相互依赖的。所以我们要最小化出现故障的几率，限制故障所造成的负面影响。我们需要良好的变更管理策略和自动回滚机制

例如，在部署新代码时，或者在对配置做出变更时，要先在一小部分服务实例上进行，然后监控它们，一旦发现关键性度量指标出现异常，马上自动回滚

另一个解决方案就是运行两套生产环境。在部署的时候只部署到其中一个生产环境，只有在确认这个环境没问题了之后才能将负载均衡器指向这个环境。这种部署方式被称为蓝绿部署或者红黑部署。

回退代码并不是件坏事。你总不可能一边把有问题的代码留在生产环境里，一边想着到底发生了什么问题。所以，在必要的时候回退代码，越快越好。

**健康监测和负载均衡**

服务实例总是因为各种原因（故障、部署或自动伸缩）经历着启动、重启、停止这样的过程。这个过程会让服务暂时或永久地不可用，为了避免出现问题，负载均衡器需要忽略出现问题的服务实例，因为它们已经不具备为用户或其他子系统提供服务的能力。

应用的健康状态可以通过外部的观察来获得，比如通过不断重复地调用/health端点来得知应用的状态，或者让应用报告自己的状态。服务发现机制会持续地收集服务实例的健康信息，负载均衡器应该被配置成只将流量路由给健康的服务实例。

**重试**

在应用程序里添加重试逻辑的时候要十分小心，因为大量的重试操作会让事情变得更糟糕，甚至导致应用程序无法从故障中恢复。

在分布式系统中，一个微服务系统可能会触发多个请求或重试操作，从而发生级联效应。为了降低重试带来的影响，应该要限制重试的次数，可以使用指数退避（exponential backoff）算法来逐步增加重试之间的延迟，直到达到重试的上限。

### 微服务架构业务切分

从事微服务架构工作的架构师，相比传统架构的架构师而言，所要求的技能更加全面。他们不仅仅是系统架构师，也是业务分析师，他们的责任重大且挑战艰巨。

从大的方向来看，微服务架构师需要具备以下基本职责。

1. 分析业务需求并切分微服务边界。 
2. 定义架构规范与文档标准。 
3. 确保微服务架构顺利落地。 
4. 改善微服务架构并提高开发效率。

职责与挑战往往是无法分离的，微服务架构师必须面对并克服这些挑战。

1. 架构需要适应不断变化的业务需求。 
2. 架构具备稳定性、扩展性、安全性、容错性等。 
3. 使技术团队深刻理解微服务思想。 
4. 展现微服务架构的价值。

我们认为，传统架构师转型为微服务架构首先需要做到的是深刻理解业务，而不是表面上了解需求。业务和需求其实是两码事，业务的背后反映了客户的刚需，而需求往往是产品经理根据业务刚需所指定的解决方案。作为微服务架构师，我们需要透过需求的表象去理解业务的本质。其次需要做到的是不断优化架构，让架构变得更加简单，更加轻量级

经过大量的微服务实践，我们总结了以下五个步骤，可帮助大家有效地切分微服务边界。

第一步：梳理业务流程。 在切分微服务之前，我们要做的第一件事情就是梳理业务流程。不妨找业务专家咨询，通过与他们沟通从而了解真实的业务流程，并将其绘制成流程图。对于过于复杂的业务流程，我们也可单独绘制流程图，并增加相关的流程说明。当然也能提供相应的状态图，用于说明业务流程中所涉及状态的变化过程。

花再多时间去分析业务流程都不过分，现在所花的每一分钟都是相当值得的。

第二步：抽取公共服务。 在业务流程中与业务不太相关的部分，我们可考虑将其剥离出来，并形成公共服务。例如，邮件发送、文件上传、其他第三方接口等。每种公共服务都对应一个微服务，每个微服务都有相关API，每个API都有自己的输入与输出。这些API一定要形成文档，以便其他服务调用。

一般情况下，抽取的公共服务都不太会变化，我们一定要想办法将不变的东西从可变的世界中抽取出来。

第三步：定义业务服务。 当公共服务抽取完毕后，业务流程中剩下来的部分就是业务服务了。建议刚开始实施微服务时，不要将业务服务的边界切得太细，可以考虑先“大切几块”，但需要确保每个服务之间尽量不要有依赖关系。换句话说，每个服务都是独立的，虽然此时服务的块头可能比较大。

我们先确保这些大块头服务可以运行在微服务基础设施上，再不断将它们进行细化，拆解为更小的服务。

第四步：设计数据模型。 深入到每个业务服务中，我们首先要做的是定义它底层所涉及的数据模型，也称为“领域模型”。此时会涉及数据库表结构设计，以及数据模型与关系设计。在数据层面上的设计是至关重要的，如果该部分设计得不到位，将增加后期实现微服务的成本。 数据模型的设计同样也需要进行文档化，这些文档将指导后端工程师顺利地完成微服务实现。

第五步：定义服务接口。 底层的数据模型设计完毕后，我们将视角转换到顶层的服务接口上。服务接口实际上就是一组API，这些API需做到职责单一，而且需要通过名称就能识别出它的业务含义。建议确保每个API的命名是全局唯一的，也建议每个API都有各自的版本号，版本号可以用自增长的方式来体现。

服务接口也需要进行文档化，这些文档一般由后端工程师编写，并提供给前端与测试工程师阅读

参考：
1. 如何设计高可用的微服务架构 
2. 轻量级微服务架构及最佳实践
3. 实施微服务，我们需要哪些基础框架？ 

